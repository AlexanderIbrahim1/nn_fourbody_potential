Seed: 0
Layer sizes: [8, 16, 16, 8]
Learning rate: 0.000200
Weight decay: 0.000100
Training data set size: 5000
Total epochs: 6000
Batch size: 2000
Batch normalization applied: False
The transformations used are the following:

ReciprocalTransformer()

StandardizeTransformer(init_pair=(0.2222222222222222, 0.45454545454545453), final_pair=(0.0, 1.0), linear_func=<function StandardizeTransformer.__post_init__.<locals>.map_init_pair_to_final_pair at 0x7f0edca0edc0>)

MinimumPermutationTransformer(less_than_comparator=LessThanEpsilon(epsilon=0.0001))

Other information:
_small8_withfast
