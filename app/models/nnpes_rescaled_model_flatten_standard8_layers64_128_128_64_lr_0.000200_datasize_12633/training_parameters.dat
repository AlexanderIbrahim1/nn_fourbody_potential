Seed: 0
Layer sizes: [64, 128, 128, 64]
Learning rate: 0.000200
Weight decay: 0.000010
Training data set size: 12633
Total epochs: 10000
Batch size: 512
Batch normalization applied: False
The transformations used are the following:

ReciprocalTransformer()

StandardizeTransformer(init_pair=(0.0, 0.45454545454545453), final_pair=(0.0, 1.0), linear_func=<function StandardizeTransformer.__post_init__.<locals>.map_init_pair_to_final_pair at 0x7f490cf75310>)

MinimumPermutationTransformer(less_than_comparator=LessThanEpsilon(epsilon=0.0001))

Other information:
_rescaled_model_flatten_standard8
